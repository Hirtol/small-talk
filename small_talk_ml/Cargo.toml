[package]
name = "small_talk_ml"
version = "0.1.0"
edition = "2021"

[lib]
doctest = false
path = "src/lib.rs"

[[bin]]
path = "src/main.rs"
name = "small_talk_ml"

[features]
default = []
cuda = ["llama-cpp-2/cuda", "llama-cpp-sys-2/cuda"]
import = ["burn-import"]

[dependencies]
tracing.workspace = true
eyre.workspace = true
thiserror.workspace = true
error_set.workspace = true
self_cell = "1.0.4"
itertools.workspace = true

# ML, llama for efficient CPU embeddings
llama-cpp-2 = { git = "https://github.com/utilityai/llama-cpp-rs", rev = "a103be60d05885fcae99665a397e72ba3533c925", features = [] }
llama-cpp-sys-2 = { git = "https://github.com/utilityai/llama-cpp-rs", rev = "a103be60d05885fcae99665a397e72ba3533c925", features = [] }
whisper-rs = { git = "https://github.com/Hirtol/whisper-rs", features = ["whisper-cpp-tracing"]}
burn = { version = "0.16.0", features = ["autodiff", "wgpu", "train", "ndarray"] }
burn-import = { version = "0.16.0", optional = true }

# Audio stuff
samplerate = "0.2.4"
wavers.workspace = true

serde.workspace = true
serde_json.workspace = true